{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1qtuz2hFkolrEWh_Z1Rrkjz_qb6DFxb9V","authorship_tag":"ABX9TyNxw85BYsRLwcRFrL7YdfCZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install segmentation_models_pytorch"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E6D_NNFbiNK7","executionInfo":{"status":"ok","timestamp":1708273383729,"user_tz":-540,"elapsed":10208,"user":{"displayName":"황지민","userId":"01545126109009120040"}},"outputId":"4352bf15-b982-4844-9cb0-83f22dce6bb8"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting segmentation_models_pytorch\n","  Downloading segmentation_models_pytorch-0.3.3-py3-none-any.whl (106 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (0.16.0+cu121)\n","Collecting pretrainedmodels==0.7.4 (from segmentation_models_pytorch)\n","  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting efficientnet-pytorch==0.7.1 (from segmentation_models_pytorch)\n","  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting timm==0.9.2 (from segmentation_models_pytorch)\n","  Downloading timm-0.9.2-py3-none-any.whl (2.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (4.66.2)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (9.4.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2.1.0+cu121)\n","Collecting munch (from pretrainedmodels==0.7.4->segmentation_models_pytorch)\n","  Downloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm==0.9.2->segmentation_models_pytorch) (6.0.1)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from timm==0.9.2->segmentation_models_pytorch) (0.20.3)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm==0.9.2->segmentation_models_pytorch) (0.4.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (1.25.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (2.31.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (4.9.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2.1.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm==0.9.2->segmentation_models_pytorch) (23.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (2024.2.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (1.3.0)\n","Building wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n","  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16428 sha256=ae0ee79de2363a3b095f45f68b29ed83b9fb1e9b7eb38133bbd053c6c30ca2f1\n","  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\n","  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60945 sha256=c9412b219064768972cd0073e92fb5734debad6bea1333ad865c02d7a68926c1\n","  Stored in directory: /root/.cache/pip/wheels/35/cb/a5/8f534c60142835bfc889f9a482e4a67e0b817032d9c6883b64\n","Successfully built efficientnet-pytorch pretrainedmodels\n","Installing collected packages: munch, efficientnet-pytorch, timm, pretrainedmodels, segmentation_models_pytorch\n","Successfully installed efficientnet-pytorch-0.7.1 munch-4.0.0 pretrainedmodels-0.7.4 segmentation_models_pytorch-0.3.3 timm-0.9.2\n"]}]},{"cell_type":"code","source":["import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","from torchvision import transforms, models\n","from PIL import Image\n","import os\n","import numpy as np\n","import random\n","from sklearn.metrics import accuracy_score\n","import torch\n","import torch.nn as nn\n","from torchvision.models.segmentation import fcn_resnet101\n","from torchvision.models.segmentation import deeplabv3_resnet101\n","from torchvision.models.segmentation import FCN_ResNet101_Weights\n","import segmentation_models_pytorch as smp\n","\n","#증강 안했을 때 # 출력형식 다른 나머지\n","\n","def create_model(model_name, num_classes):\n","    if model_name == 'unet':\n","        model = smp.Unet(encoder_name=\"resnet34\", classes=num_classes, encoder_weights=\"imagenet\")\n","    elif model_name == 'pspnet':\n","        model = smp.PSPNet(encoder_name=\"resnet34\", classes=num_classes, encoder_weights=\"imagenet\")\n","    elif model_name == 'deeplabv3plus':\n","        model = smp.DeepLabV3Plus(encoder_name=\"resnet34\", classes=num_classes, encoder_weights=\"imagenet\")\n","    elif model_name == 'linknet':\n","        model = smp.Linknet(encoder_name=\"resnet34\", classes=num_classes, encoder_weights=\"imagenet\")\n","    else:\n","        print(\"Unsupported model: {}\".format(model_name))\n","        return None\n","    return model\n","\n","class SegmentationDataset(Dataset):\n","    def __init__(self, image_dir, label_dir, image_list, transform=None):\n","        self.image_dir = image_dir\n","        self.label_dir = label_dir\n","        self.transform = transform\n","        self.images = image_list\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        img_path = os.path.join(self.image_dir, self.images[idx])\n","        label_path = os.path.join(self.label_dir, self.images[idx].replace('.jpg', '_mask.png'))\n","        image = Image.open(img_path).convert('RGB')\n","        label = Image.open(label_path).convert('L')\n","        if self.transform:\n","            image = self.transform(image)\n","            label = self.transform(label) * 255\n","        return image, label.squeeze().long()\n","\n","def split_dataset(image_dir, test_ratio=0.2):\n","    images = os.listdir(image_dir)\n","    random.shuffle(images)\n","    split_idx = int(len(images) * (1 - test_ratio))\n","    return images[:split_idx], images[split_idx:]\n","\n","test_transforms = transforms.Compose([\n","    transforms.Resize((256, 256)),\n","    transforms.ToTensor(),\n","])\n","train_transforms = transforms.Compose([\n","    transforms.Resize((256, 256)),\n","    transforms.ToTensor(),\n","])\n","\n","image_dir = '/content/drive/MyDrive/Colab Notebooks/segmentation/images'\n","label_dir = '/content/drive/MyDrive/Colab Notebooks/segmentation/labels'\n","train_images, test_images = split_dataset(image_dir)\n","train_dataset = SegmentationDataset(image_dir=image_dir, label_dir=label_dir, image_list=train_images, transform=train_transforms)\n","test_dataset = SegmentationDataset(image_dir, label_dir, test_images, transform=test_transforms)\n","train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model_names = [\"unet\", \"pspnet\", \"deeplabv3plus\", \"linknet\"]\n","num_classes = 4\n","\n","for model_name in model_names:\n","    model = create_model(model_name, num_classes).to(device)\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=0.001)\n","    num_epochs = 25\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        for images, labels in train_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            optimizer.zero_grad()\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","        print(f'{model_name} - Epoch {epoch+1}, Loss: {loss.item()}')\n","\n","    model.eval()\n","    all_preds = []\n","    all_labels = []\n","    with torch.no_grad():\n","        for images, labels in test_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n","            labels = labels.cpu().numpy()\n","            all_preds.extend(preds.flatten())\n","            all_labels.extend(labels.flatten())\n","    accuracy = accuracy_score(all_labels, all_preds)\n","    print(f'{model_name} - Test Accuracy: {accuracy}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5tpPIi94CVs6","executionInfo":{"status":"ok","timestamp":1708277582313,"user_tz":-540,"elapsed":1305620,"user":{"displayName":"황지민","userId":"01545126109009120040"}},"outputId":"0883924c-15bb-4f29-cf0d-7231e4c53788"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["unet - Epoch 1, Loss: 0.4258987009525299\n","unet - Epoch 2, Loss: 0.3520199954509735\n","unet - Epoch 3, Loss: 0.3343725800514221\n","unet - Epoch 4, Loss: 0.7797642946243286\n","unet - Epoch 5, Loss: 0.1825096309185028\n","unet - Epoch 6, Loss: 0.14660552144050598\n","unet - Epoch 7, Loss: 0.15887451171875\n","unet - Epoch 8, Loss: 0.1435546576976776\n","unet - Epoch 9, Loss: 0.17108339071273804\n","unet - Epoch 10, Loss: 0.19695845246315002\n","unet - Epoch 11, Loss: 0.11237483471632004\n","unet - Epoch 12, Loss: 0.16465617716312408\n","unet - Epoch 13, Loss: 0.08306649327278137\n","unet - Epoch 14, Loss: 0.08586934208869934\n","unet - Epoch 15, Loss: 0.08522182703018188\n","unet - Epoch 16, Loss: 0.11206576228141785\n","unet - Epoch 17, Loss: 0.08915679156780243\n","unet - Epoch 18, Loss: 0.08836173266172409\n","unet - Epoch 19, Loss: 0.07134784758090973\n","unet - Epoch 20, Loss: 0.05095013603568077\n","unet - Epoch 21, Loss: 0.07222555577754974\n","unet - Epoch 22, Loss: 0.056603118777275085\n","unet - Epoch 23, Loss: 0.28968507051467896\n","unet - Epoch 24, Loss: 0.11879944801330566\n","unet - Epoch 25, Loss: 0.07456750422716141\n","unet - Test Accuracy: 0.948850440979004\n","pspnet - Epoch 1, Loss: 0.2998427152633667\n","pspnet - Epoch 2, Loss: 0.279326468706131\n","pspnet - Epoch 3, Loss: 0.2539810836315155\n","pspnet - Epoch 4, Loss: 0.1589643657207489\n","pspnet - Epoch 5, Loss: 0.17567266523838043\n","pspnet - Epoch 6, Loss: 0.16526037454605103\n","pspnet - Epoch 7, Loss: 0.17240825295448303\n","pspnet - Epoch 8, Loss: 0.1412930190563202\n","pspnet - Epoch 9, Loss: 0.13755491375923157\n","pspnet - Epoch 10, Loss: 0.11622771620750427\n","pspnet - Epoch 11, Loss: 0.09438227117061615\n","pspnet - Epoch 12, Loss: 0.0938095897436142\n","pspnet - Epoch 13, Loss: 0.09569787234067917\n","pspnet - Epoch 14, Loss: 0.07031558454036713\n","pspnet - Epoch 15, Loss: 0.09017685055732727\n","pspnet - Epoch 16, Loss: 0.14110025763511658\n","pspnet - Epoch 17, Loss: 0.11221232265233994\n","pspnet - Epoch 18, Loss: 0.06616392731666565\n","pspnet - Epoch 19, Loss: 0.07214656472206116\n","pspnet - Epoch 20, Loss: 0.09712591022253036\n","pspnet - Epoch 21, Loss: 0.08193780481815338\n","pspnet - Epoch 22, Loss: 0.06912485510110855\n","pspnet - Epoch 23, Loss: 0.06998451054096222\n","pspnet - Epoch 24, Loss: 0.07855389267206192\n","pspnet - Epoch 25, Loss: 0.07115975022315979\n","pspnet - Test Accuracy: 0.9464645385742188\n","deeplabv3plus - Epoch 1, Loss: 0.4271559715270996\n","deeplabv3plus - Epoch 2, Loss: 0.28012844920158386\n","deeplabv3plus - Epoch 3, Loss: 0.242892324924469\n","deeplabv3plus - Epoch 4, Loss: 0.17642377316951752\n","deeplabv3plus - Epoch 5, Loss: 0.19153708219528198\n","deeplabv3plus - Epoch 6, Loss: 0.2328604906797409\n","deeplabv3plus - Epoch 7, Loss: 0.12824761867523193\n","deeplabv3plus - Epoch 8, Loss: 0.1266840100288391\n","deeplabv3plus - Epoch 9, Loss: 0.08851024508476257\n","deeplabv3plus - Epoch 10, Loss: 0.09663158655166626\n","deeplabv3plus - Epoch 11, Loss: 0.10377036035060883\n","deeplabv3plus - Epoch 12, Loss: 0.15832552313804626\n","deeplabv3plus - Epoch 13, Loss: 0.1247190535068512\n","deeplabv3plus - Epoch 14, Loss: 0.10441526025533676\n","deeplabv3plus - Epoch 15, Loss: 0.08020228147506714\n","deeplabv3plus - Epoch 16, Loss: 0.06455327570438385\n","deeplabv3plus - Epoch 17, Loss: 0.07768017053604126\n","deeplabv3plus - Epoch 18, Loss: 0.06410756707191467\n","deeplabv3plus - Epoch 19, Loss: 0.09459605813026428\n","deeplabv3plus - Epoch 20, Loss: 0.06922327727079391\n","deeplabv3plus - Epoch 21, Loss: 0.06148120388388634\n","deeplabv3plus - Epoch 22, Loss: 0.05507930368185043\n","deeplabv3plus - Epoch 23, Loss: 0.10169511288404465\n","deeplabv3plus - Epoch 24, Loss: 0.06471963226795197\n","deeplabv3plus - Epoch 25, Loss: 0.06748918443918228\n","deeplabv3plus - Test Accuracy: 0.9569032669067383\n","linknet - Epoch 1, Loss: 0.49516624212265015\n","linknet - Epoch 2, Loss: 0.4706314504146576\n","linknet - Epoch 3, Loss: 0.20894429087638855\n","linknet - Epoch 4, Loss: 0.23762579262256622\n","linknet - Epoch 5, Loss: 0.22591392695903778\n","linknet - Epoch 6, Loss: 0.1681462526321411\n","linknet - Epoch 7, Loss: 0.12694045901298523\n","linknet - Epoch 8, Loss: 0.15535126626491547\n","linknet - Epoch 9, Loss: 0.158433198928833\n","linknet - Epoch 10, Loss: 0.1146794930100441\n","linknet - Epoch 11, Loss: 0.12764087319374084\n","linknet - Epoch 12, Loss: 0.08350850641727448\n","linknet - Epoch 13, Loss: 0.07758526504039764\n","linknet - Epoch 14, Loss: 0.08442367613315582\n","linknet - Epoch 15, Loss: 0.07720737159252167\n","linknet - Epoch 16, Loss: 0.06461201608181\n","linknet - Epoch 17, Loss: 0.05387009680271149\n","linknet - Epoch 18, Loss: 0.0414634570479393\n","linknet - Epoch 19, Loss: 0.06404012441635132\n","linknet - Epoch 20, Loss: 0.09631626307964325\n","linknet - Epoch 21, Loss: 0.05294587463140488\n","linknet - Epoch 22, Loss: 0.05750405788421631\n","linknet - Epoch 23, Loss: 0.04681440442800522\n","linknet - Epoch 24, Loss: 0.14734885096549988\n","linknet - Epoch 25, Loss: 0.06742946058511734\n","linknet - Test Accuracy: 0.9625320434570312\n"]}]},{"cell_type":"code","source":["import torchvision.transforms as T\n","from torchvision.models.detection import maskrcnn_resnet50_fpn\n","from PIL import Image, ImageDraw\n","import numpy as np\n","import torch\n","\n","# 사전 훈련된 Mask R-CNN 모델 불러오기\n","model = maskrcnn_resnet50_fpn(pretrained=True)\n","model.eval()\n","\n","# 클래스 라벨\n","COCO_INSTANCE_CATEGORY_NAMES = [\n","    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n","    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n","    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n","    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n","    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n","    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n","    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n","    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n","    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n","    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n","    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n","    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n","]\n","\n","# 객체 감지 및 분할 함수\n","def detect_objects(image_path):\n","    # 이미지 불러오기\n","    img = Image.open(image_path)\n","\n","    # 전처리\n","    transform = T.Compose([T.ToTensor()])\n","    img_tensor = transform(img)\n","\n","    # 객체 감지 및 분할\n","    with torch.no_grad():\n","        prediction = model([img_tensor])[0]\n","\n","    # 결과 추출\n","    masks = prediction['masks']\n","    labels = prediction['labels']\n","    scores = prediction['scores']\n","\n","    # 추론 결과를 필터링하여 반환\n","    detections = []\n","    for mask, label, score in zip(masks, labels, scores):\n","        # 신뢰도가 일정 수준 이상인 경우에만 결과에 추가\n","        if score >= 0.5:\n","            mask = mask[0]  # 배치 차원 제거\n","            detections.append({'mask': mask, 'label': COCO_INSTANCE_CATEGORY_NAMES[label.item()], 'score': score.item()})\n","\n","    return detections\n","# 객체 감지 및 분할 함수\n","def detect_objects(image_path):\n","    # 이미지 불러오기\n","    img = Image.open(image_path)\n","\n","    # 전처리\n","    transform = T.Compose([T.ToTensor()])\n","    img_tensor = transform(img)\n","\n","    # 객체 감지 및 분할\n","    with torch.no_grad():\n","        prediction = model([img_tensor])[0]\n","\n","    # 결과 추출\n","    masks = prediction['masks']\n","    labels = prediction['labels']\n","    scores = prediction['scores']\n","    boxes = prediction['boxes']\n","\n","    # 추론 결과를 필터링하여 반환\n","    detections = []\n","    for mask, label, score, box in zip(masks, labels, scores, boxes):\n","        # 신뢰도가 일정 수준 이상인 경우에만 결과에 추가\n","        if score >= 0.5:\n","            mask = mask[0]  # 배치 차원 제거\n","            detections.append({'mask': mask, 'label': COCO_INSTANCE_CATEGORY_NAMES[label.item()], 'score': score.item(), 'box': box.tolist()})\n","\n","    return detections\n","# 객체 감지 및 분할 함수\n","def detect_objects(image_path):\n","    # 이미지 불러오기\n","    img = Image.open(image_path)\n","\n","    # 전처리\n","    transform = T.Compose([T.ToTensor()])\n","    img_tensor = transform(img)\n","\n","    # 객체 감지 및 분할\n","    with torch.no_grad():\n","        prediction = model([img_tensor])[0]\n","\n","    # 결과 추출\n","    masks = prediction['masks']\n","    labels = prediction['labels']\n","    scores = prediction['scores']\n","    boxes = prediction['boxes']\n","\n","    # 추론 결과를 필터링하여 반환 (사람만)\n","    detections = []\n","    for mask, label, score, box in zip(masks, labels, scores, boxes):\n","        # 신뢰도가 일정 수준 이상이고 라벨이 사람인 경우에만 결과에 추가\n","        if score >= 0.5 and label == 1:  # 1은 사람에 해당하는 라벨\n","            mask = mask[0]  # 배치 차원 제거\n","            detections.append({'mask': mask, 'label': COCO_INSTANCE_CATEGORY_NAMES[label.item()], 'score': score.item(), 'box': box.tolist()})\n","\n","    return detections\n","def convert_box_to_center(box, image_width, image_height):\n","    # 박스 좌표를 중심점 좌표와 너비, 높이로 변환\n","    x_center = ((box[0] + box[2]) / 2) / image_width\n","    y_center = ((box[1] + box[3]) / 2) / image_height\n","    width = (box[2] - box[0]) / image_width\n","    height = (box[3] - box[1]) / image_height\n","    return x_center, y_center, width, height\n","\n","# 객체 감지 및 분할 결과 텍스트로 출력 함수 (사람만)\n","def print_detection_results(detections, image_width, image_height):\n","    for i, detection in enumerate(detections, start=1):\n","        label = detection['label']\n","        score = detection['score']\n","        box = detection['box']\n","\n","        # 중심점 좌표와 너비, 높이 계산\n","        x_center, y_center, width, height = convert_box_to_center(box, image_width, image_height)\n","\n","        print(f\"{x_center:f} {y_center:f} {width:f} {height:f}\")\n","\n","# 이미지 파일 경로\n","image_path = \"/content/drive/MyDrive/Colab Notebooks/detection/images/240116133524-0043.jpg\"\n","\n","# 이미지 불러오기\n","img = Image.open(image_path)\n","image_width, image_height = img.size\n","\n","# 객체 감지 및 분할 수행\n","detections = detect_objects(image_path)\n","\n","# 객체 감지 및 분할 결과 텍스트로 출력 (사람만)\n","print_detection_results(detections, image_width, image_height)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o4iV42sRYgD2","executionInfo":{"status":"ok","timestamp":1708190257938,"user_tz":-540,"elapsed":9063,"user":{"displayName":"황지민","userId":"01545126109009120040"}},"outputId":"5e47e2c8-313d-476a-a85c-7cb49bc17cff"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.448560 0.475042 0.016446 0.064658\n","0.319735 0.490922 0.013721 0.063306\n","0.655321 0.286448 0.010210 0.047385\n","0.133461 0.323387 0.009756 0.042698\n","0.304779 0.257048 0.009210 0.047592\n","0.427931 0.150444 0.007344 0.032236\n","0.159207 0.493104 0.005928 0.032146\n","0.192017 0.329184 0.012821 0.037121\n"]}]},{"cell_type":"code","source":["import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","from torchvision import transforms, models\n","from PIL import Image\n","import os\n","import numpy as np\n","import random\n","from sklearn.metrics import accuracy_score\n","import torch\n","import torch.nn as nn\n","from torchvision.models.segmentation import fcn_resnet101\n","from torchvision.models.segmentation import deeplabv3_resnet101\n","from torchvision.models.segmentation import FCN_ResNet101_Weights\n","import segmentation_models_pytorch as smp\n","\n","def create_model(model_name, num_classes):\n","    if model_name == 'unet':\n","        model = smp.Unet(encoder_name=\"resnet34\", classes=num_classes, encoder_weights=\"imagenet\")\n","    elif model_name == 'pspnet':\n","        model = smp.PSPNet(encoder_name=\"resnet34\", classes=num_classes, encoder_weights=\"imagenet\")\n","    elif model_name == 'deeplabv3plus':\n","        model = smp.DeepLabV3Plus(encoder_name=\"resnet34\", classes=num_classes, encoder_weights=\"imagenet\")\n","    elif model_name == 'linknet':\n","        model = smp.Linknet(encoder_name=\"resnet34\", classes=num_classes, encoder_weights=\"imagenet\")\n","    else:\n","        print(\"Unsupported model: {}\".format(model_name))\n","        return None\n","    return model\n","\n","class SegmentationDataset(Dataset):\n","    def __init__(self, image_dir, label_dir, image_list, transform=None):\n","        self.image_dir = image_dir\n","        self.label_dir = label_dir\n","        self.transform = transform\n","        self.images = image_list\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        img_path = os.path.join(self.image_dir, self.images[idx])\n","        label_path = os.path.join(self.label_dir, self.images[idx].replace('.jpg', '_mask.png'))\n","        image = Image.open(img_path).convert('RGB')\n","        label = Image.open(label_path).convert('L')\n","        if self.transform:\n","            image = self.transform(image)\n","            label = self.transform(label) * 255\n","        return image, label.squeeze().long()\n","\n","def split_dataset(image_dir, test_ratio=0.2):\n","    images = os.listdir(image_dir)\n","    random.shuffle(images)\n","    split_idx = int(len(images) * (1 - test_ratio))\n","    return images[:split_idx], images[split_idx:]\n","\n","test_transforms = transforms.Compose([\n","    transforms.Resize((256, 256)),\n","    transforms.ToTensor(),\n","])\n","train_transforms = transforms.Compose([\n","    transforms.Resize((256, 256)),\n","    transforms.ToTensor(),\n","])\n","\n","image_dir = '/content/drive/MyDrive/Colab Notebooks/segmentation/images'\n","label_dir = '/content/drive/MyDrive/Colab Notebooks/segmentation/labels'\n","train_images, test_images = split_dataset(image_dir)\n","train_dataset = SegmentationDataset(image_dir=image_dir, label_dir=label_dir, image_list=train_images, transform=train_transforms)\n","test_dataset = SegmentationDataset(image_dir, label_dir, test_images, transform=test_transforms)\n","train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model_names = [\"unet\", \"pspnet\", \"deeplabv3plus\", \"linknet\"]\n","num_classes = 4\n","\n","for model_name in model_names:\n","    model = create_model(model_name, num_classes).to(device)\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","    prev_loss = float('inf')\n","    patience = 5\n","    no_improvement_count = 0\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        for images, labels in train_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            optimizer.zero_grad()\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","        print(f'{model_name} - Epoch {epoch+1}, Loss: {loss.item()}')\n","\n","        # 검증 데이터셋을 사용하여 손실을 계산하고, 조기 종료를 확인\n","        model.eval()\n","        with torch.no_grad():\n","            val_loss = 0\n","            for images, labels in test_loader:\n","                images, labels = images.to(device), labels.to(device)\n","                outputs = model(images)\n","                loss = criterion(outputs, labels)\n","                val_loss += loss.item() * images.size(0)\n","            val_loss /= len(test_loader.dataset)\n","\n","        # 손실이 이전 손실보다 증가했는지 확인하여 조기 종료 조건을 적용\n","        if val_loss >= prev_loss:\n","            no_improvement_count += 1\n","            if no_improvement_count >= patience:\n","                print(f'Early stopping at epoch {epoch+1} due to no improvement in validation loss.')\n","                break\n","        else:\n","            no_improvement_count = 0\n","            prev_loss = val_loss\n","\n","    model.eval()\n","    all_preds = []\n","    all_labels = []\n","    with torch.no_grad():\n","        for images, labels in test_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n","            labels = labels.cpu().numpy()\n","            all_preds.extend(preds.flatten())\n","            all_labels.extend(labels.flatten())\n","    accuracy = accuracy_score(all_labels, all_preds)\n","    print(f'{model_name} - Test Accuracy: {accuracy}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":942},"id":"XW1ZcxbOBUP8","executionInfo":{"status":"error","timestamp":1708278296954,"user_tz":-540,"elapsed":478972,"user":{"displayName":"황지민","userId":"01545126109009120040"}},"outputId":"74ef3151-6e5c-4e00-b033-767d19717e33"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["unet - Epoch 1, Loss: 0.5311014652252197\n","unet - Epoch 2, Loss: 0.2736879587173462\n","unet - Epoch 3, Loss: 0.675195574760437\n","unet - Epoch 4, Loss: 0.2520984411239624\n","unet - Epoch 5, Loss: 0.18400780856609344\n","unet - Epoch 6, Loss: 0.222054123878479\n","unet - Epoch 7, Loss: 0.12370148301124573\n","unet - Epoch 8, Loss: 0.08995825797319412\n","unet - Epoch 9, Loss: 0.12083162367343903\n","unet - Epoch 10, Loss: 0.1239992007613182\n","unet - Epoch 11, Loss: 0.14760400354862213\n","unet - Epoch 12, Loss: 0.15693125128746033\n","unet - Epoch 13, Loss: 0.13098283112049103\n","unet - Epoch 14, Loss: 0.09710965305566788\n","unet - Epoch 15, Loss: 0.1107923835515976\n","unet - Epoch 16, Loss: 0.071767657995224\n","unet - Epoch 17, Loss: 0.08650293201208115\n","unet - Epoch 18, Loss: 0.06359551101922989\n","unet - Epoch 19, Loss: 0.0715545117855072\n","unet - Epoch 20, Loss: 0.08093225210905075\n","unet - Epoch 21, Loss: 0.06656844913959503\n","unet - Epoch 22, Loss: 0.06671416759490967\n","unet - Epoch 23, Loss: 0.3909984529018402\n","unet - Epoch 24, Loss: 0.03744671121239662\n","unet - Epoch 25, Loss: 0.05287373811006546\n","unet - Test Accuracy: 0.9660423278808594\n","pspnet - Epoch 1, Loss: 0.421109139919281\n","pspnet - Epoch 2, Loss: 0.24553006887435913\n","pspnet - Epoch 3, Loss: 0.18407227098941803\n","pspnet - Epoch 4, Loss: 0.12796275317668915\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-a8ae8ec30961>\u001b[0m in \u001b[0;36m<cell line: 77>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mval_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/segmentation_models_pytorch/base/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_input_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mdecoder_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/segmentation_models_pytorch/encoders/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_depth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mused\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnormalization\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval\u001b[0m \u001b[0mmode\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[0mare\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \"\"\"\n\u001b[0;32m--> 171\u001b[0;31m         return F.batch_norm(\n\u001b[0m\u001b[1;32m    172\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0;31m# If buffers are not to be tracked, ensure that they won't be updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0m_verify_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2478\u001b[0;31m     return torch.batch_norm(\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m     )\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"bqLl4owtBXg-"},"execution_count":null,"outputs":[]}]}